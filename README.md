# model-evalution-metrics
A practical guide to machine learning model evaluation metrics: what they mean, when to use them, and common pitfalls across classification, regression, and forecasting.

Why this repository exists

In real machine learning interviews (especially Google / Meta / Amazon), the question is not:

“What is F1-score?”

The real question is:

“Why did you choose this metric over others, and what trade-offs does it introduce?”

This repository is built to answer exactly that.

It focuses on:

What each evaluation metric means

When to use it

When not to use it

How metric choice impacts real-world decisions

What you’ll learn (Interview-Relevant)

By the end of this repo, you should be able to confidently explain:

Why accuracy is misleading for imbalanced data

When precision > recall (and vice versa)

Why ROC-AUC can look good while the model is bad

How to choose metrics for:

Fraud detection

Trust & Safety

Healthcare

Forecasting

Ranking / recommendation systems

How to defend your metric choice in a Google-style interview
